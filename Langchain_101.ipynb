{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1gyBb5EPM6qHMpmxeUBXHhFfYlGVSa219",
      "authorship_tag": "ABX9TyMpyHDFCcLTv6qRp2lhzswU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakyurek/llm/blob/main/Langchain_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation & Prerequisites"
      ],
      "metadata": {
        "id": "h0TAdZWxSycB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqTi69wAP1wE",
        "outputId": "849414dd-8ccc-4c8b-b5f5-583b783014f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.8/516.8 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q openai langchain python-dotenv deeplake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "n-V2WOfhP5Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/Colab Notebooks\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQStSo-mkgo4",
        "outputId": "a74192a0-6c01-47e1-fca5-7b7ccb195ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp env .env"
      ],
      "metadata": {
        "id": "CJfpcOLCchwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(\"/content/drive/MyDrive/env\")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "Feh4BEuCQIps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API"
      ],
      "metadata": {
        "id": "ByiwSKXDS6MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "2mZBxJVSRUyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before executing the following code, make sure to have\n",
        "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)"
      ],
      "metadata": {
        "id": "QxhYlAVCQaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhdJzakBRWNn",
        "outputId": "38a70050-4115-4a45-f533-4210dfa9e0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "1. 30-minute outdoor jog - Start each day with a 30-minute outdoor jog at a moderate pace to improve overall cardiovascular endurance.\n",
            "\n",
            "2. Interval training - Spend two days per week engaging in interval training, alternating between sprints and slow jogs. Refuse to stop and push yourself to improve your times.\n",
            "\n",
            "3. Hill sprints - Find a hill near your outdoor running spot and incorporate it into your routine. Perform hill sprints two to three times a week to raise your heart rate and build endurance.\n",
            "\n",
            "4. Plyometrics - Incorporate plyometric exercises, such as box jumps and jump squats, into your routine once or twice a week. These exercises will help increase your overall power and explosiveness.\n",
            "\n",
            "5. Take rest days - Make sure to take at least one to two days off per week to rest and recover. This will help you avoid injury and ensure that your body is ready for the next workout.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain"
      ],
      "metadata": {
        "id": "azOdVR5CTDN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "xb4x9rl1TF27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"product\"],\n",
        "    template = \"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n"
      ],
      "metadata": {
        "id": "EU3Q8o2ETYC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Motorbike\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "urEmgoVRT1pl",
        "outputId": "01e16e72-7bb8-4419-f7e9-e9c33a53dfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nCycleMaster Motors.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "Ngq1MUWXVyIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\",temperature=.0)\n",
        "conservation = ConversationChain(\n",
        "    llm = llm,\n",
        "    verbose = True,\n",
        "    memory = ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "conservation.predict(input=\"Tell me about yourself\")\n",
        "conservation.predict(input=\"What can you do?\")\n",
        "print(conservation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2orUrM61T5Ei",
        "outputId": "5afb17ab-2cc8-4cc3-fda2-01985b20b94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself\n",
            "AI Assistant:  Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! My name is AI and I'm a virtual assistant. I'm here to help you with any questions you may have. I'm powered by artificial intelligence and I'm constantly learning new things. I'm also able to provide you with information from my context, such as the current weather, news, and more. What can I help you with today?\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I can help you with a variety of tasks. I can provide you with information from my context, such as the current weather, news, and more. I can also help you with scheduling tasks, setting reminders, and more. I'm constantly learning new things, so I'm always able to help you with more tasks.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI Assistant', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepLake"
      ],
      "metadata": {
        "id": "XEROjG3hc5Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSDav9wqk2NU",
        "outputId": "b7c90e9a-99ca-4964-e820-ddd7b22f4ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DB creation"
      ],
      "metadata": {
        "id": "ClPP6njp_-hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "B2Z9aXrWc8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "texts = [\n",
        "    \"Napoleon Bonparte was born in 15 August 1769\",\n",
        "    \"Louis XIV was born in 5 September 1638\"\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "my_activeloop_org_id = \"aliakyurek\"\n",
        "my_activeloop_dataset_name = \"langchain_course\"\n",
        "\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "db = DeepLake(dataset_path = dataset_path, embedding_function=embeddings)\n",
        "\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NodiJd1qfL0P",
        "outputId": "654338c1-382c-4ac8-c02e-b86f92ac4bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://aliakyurek/langchain_course already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://aliakyurek/langchain_course', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (2, 1536)  float32   None   \n",
            "    id        text      (2, 1)      str     None   \n",
            " metadata     json      (2, 1)      str     None   \n",
            "   text       text      (2, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6c858530-1021-11ee-a0dc-0242ac1c000c',\n",
              " '6c8586f2-1021-11ee-a0dc-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DB Querying"
      ],
      "metadata": {
        "id": "JRHyymaR4P3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "jzuPrTkdlBmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_qa.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwQukMsG3DXe",
        "outputId": "a1d26d6c-1e38-4325-982a-fd90b7a2b660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Napoleon Bonaparte was born on 15 August 1769.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Retrieval QA System\",\n",
        "        func = retrieval_qa.run,\n",
        "        description = \"Useful for answering questions.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "nuMvAggOmJxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent does 3 things:\n",
        "- Ne yapacağını anlıyor\n",
        "- Hangi action'ı seçeceğine karar veriyor\n",
        "- Seçtiği tooldan dönen yanıtı, kendi ihtiyacı olan bilgiyer göre özetliyor.\n",
        "Ve bunları yaparken LLM kullanıyor."
      ],
      "metadata": {
        "id": "7G4OJ4ENK3n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4kAIo843InM",
        "outputId": "ec770b26-36b3-4452-ee33-0d9ea22cfb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born.\n",
            "Action: Retrieval QA System\n",
            "Action Input: When was Napoleone born?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Napoleon Bonaparte was born on 15 August 1769.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DB Update"
      ],
      "metadata": {
        "id": "mAJy98NrfJWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the existing Deep Lake dataset and specify the embedding function\n",
        "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
        "\n",
        "# create new documents\n",
        "texts = [\n",
        "    \"Lady Gaga was born in 28 March 1986\",\n",
        "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# add documents to our Deep Lake dataset\n",
        "db.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR8t2zhR4X_D",
        "outputId": "c5fe1f82-b428-488f-f878-c266b3ab0697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://aliakyurek/langchain_course already exists, loading from the storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://aliakyurek/langchain_course', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape     dtype  compression\n",
            "  -------    -------    -------   -------  ------- \n",
            " embedding  embedding  (4, 1536)  float32   None   \n",
            "    id        text      (4, 1)      str     None   \n",
            " metadata     json      (4, 1)      str     None   \n",
            "   text       text      (4, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5bf16bce-102d-11ee-a0dc-0242ac1c000c',\n",
              " '5bf16d18-102d-11ee-a0dc-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(\n",
        "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "-LrxM9PU6S88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"When was Napoleone born?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STBNyjEh4d_7",
        "outputId": "57cc888b-4c05-471b-c78d-a830af28ae4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Lady Gaga was born on 28 March 1986.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_qa.run(\"When is Lady GAga Born\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ntbbl96TAund"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Google"
      ],
      "metadata": {
        "id": "B8Q7bnCb99PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, Tool, AgentType\n",
        "from langchain.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "S6CJ0wgO4sxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)"
      ],
      "metadata": {
        "id": "6tnn1uXQ-NMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
        "# Google Search via API.\n",
        "search = GoogleSearchAPIWrapper()"
      ],
      "metadata": {
        "id": "a3caXMu4-Wdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"google-search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer a question\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "PuJpsZUr-YHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True, max_iterations=6)"
      ],
      "metadata": {
        "id": "x8BmgF8o_nSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the difference between predict and run in langchain?\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl3Vmma7_zkT",
        "outputId": "9c0a9223-658a-4eae-fefb-04ad2892f646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to understand the context of these terms\n",
            "Action: google-search\n",
            "Action Input: \"difference between predict and run in langchain\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mApr 9, 2023 ... LangChain provides a generic interface for many different LLMs. Most of them work via their API but you can also run local models. To get the prediction (i.e. AI-generated text), we use run function with the name of the product . from langchain.llms import OpenAI from langchain.chains ... An LLMChain is a simple chain that adds some functionality around language ... predict is similar to run method except that the input keys are specified as ... Apr 9, 2023 ... An introduction to different conversational memory types for building chatbots and ... return is simply the predicted continuation of the conversation. 4 days ago ... How to use LangChain for building language applications, such as ChatGPT ... Now, let's get an LLM like OpenAI and predict with this model. Log a LangChain model as an MLflow artifact for the current run. ... model output (e.g. model predictions generated on the training dataset), for example:. Langchain is an open-source framework which facilitates the creation of LLM based applications ... conversation.predict(input=\"Tell me more about yourself? This module exports multivariate LangChain models in the langchain flavor and univariate ... prompt=prompt) prediction = chain.run(input_str) input_columns ... Apr 24, 2023 ... That is done in combine_docs() - ending in this call to llm_chain.predict() : return self.llm_chain.predict(callbacks=callbacks, **inputs), ... api_request_chain (langchain.chains.llm. ... Run the logic of this chain and add to output if desired. ... completion = llm.predict(adjective=\"funny\").\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: LangChain is an open-source framework which facilitates the creation of LLM based applications. The run function is used to get the prediction (i.e. AI-generated text) from the LLM, while the predict function is similar to the run method except that the input keys are specified as arguments. The predict function is used to generate a continuation of the conversation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChain is an open-source framework which facilitates the creation of LLM based applications. The run function is used to get the prediction (i.e. AI-generated text) from the LLM, while the predict function is similar to the run method except that the input keys are specified as arguments. The predict function is used to generate a continuation of the conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Tools"
      ],
      "metadata": {
        "id": "Tcamj5aJMvk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import initialize_agent,AgentType\n",
        "\n"
      ],
      "metadata": {
        "id": "w_mmIrq7Mu51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.1)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"query\"],\n",
        "    template=\"Write a summary of the following text: {query}\"\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=llm, prompt=prompt)\n"
      ],
      "metadata": {
        "id": "ipEYriWHA-Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = GoogleSearchAPIWrapper()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for finding information about recent events\"\n",
        "    ),\n",
        "    Tool(\n",
        "       name='Summarizer',\n",
        "       func=summarize_chain.run,\n",
        "       description='useful for summarizing texts'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "0RPDq5FxYRJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=\"Check your output and make sure it conforms!\"\n",
        ")"
      ],
      "metadata": {
        "id": "dDGKXL3-ZZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIctd5vtZcGA",
        "outputId": "5c4a5b1f-0d5b-4074-db2e-b83a4a77622e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for recent news about the Mars rover and then summarize the results.\n",
            "Action: Search\n",
            "Action Input: Latest news about the Mars rover\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mMars 2020 Perseverance Rover · The largest and most capable rover ever sent to Mars. ... Curiosity Rover · Measures Mars' atmosphere to understand its climate ... Dec 15, 2021 ... Mars Sample Return is going to have great stuff to choose from!” Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. Oct 19, 2022 ... NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture this panorama of a hill nicknamed ... Get the Latest JPL News. NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. A book-shaped rock on Mars was likely carved by ancient water running through the area, scientists on NASA's Curiosity rover mission say. Feb 8, 2023 ... Curiosity Rover Finds New Clues to Mars' Watery Past ... at Gediz Vallis Ridge twice last year but could only survey it from a distance. Sep 16, 2022 ... Since July, NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. Apr 28, 2023 ... A new study suggests water on Mars may be more widespread and recent than previously thought. Scientists reported the finding from China's ... Mar 7, 2023 ... 27, helps provide scientists with information about the particle sizes within the clouds. Sign up for National Breaking News Alerts.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should summarize the results of the search.\n",
            "Action: Summarizer\n",
            "Action Input: Mars 2020 Perseverance Rover is the largest and most capable rover ever sent to Mars. Curiosity Rover measures Mars' atmosphere to understand its climate. The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. NASA's Curiosity Mars rover used its Mast Camera to capture a panorama of a hill. NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. A book-shaped rock on Mars was likely carved by ancient water running through the area. Curiosity Rover Finds New Clues to Mars' Watery Past. A new study suggests water on Mars may be more widespread and recent than previously thought.\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "\n",
            "NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It is designed to look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. The Curiosity Rover, which has been on the Red Planet for more than four years, has used its Mast Camera to capture a panorama of a hill and measure Mars' atmosphere to understand its climate. A new study suggests that water on Mars may be more widespread and recent than previously thought, and a book-shaped rock on Mars was likely carved by ancient water running through the area.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It is designed to look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. The Curiosity Rover has been on the Red Planet for more than four years and has used its Mast Camera to capture a panorama of a hill and measure Mars' atmosphere to understand its climate. A new study suggests that water on Mars may be more widespread and recent than previously thought, and a book-shaped rock on Mars was likely carved by ancient water running through the area.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maMbni3JZs_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}